diff --git a/Cargo.toml b/Cargo.toml
index 6282c31..47e6b12 100644
--- a/Cargo.toml
+++ b/Cargo.toml
@@ -14,8 +14,8 @@ serde = { version = "1.0", features = [ "rc", "derive" ]}
 serde_json = "1.0"
 libc = "0.2"
 env_logger = "0.7.1"
-pyo3 = "0.18.1"
-numpy = "0.18.0"
+pyo3 = { git = "https://github.com/timfel/pyo3.git", branch = "v0.18.1.graalpy" }
+numpy = { git = "https://github.com/msimacek/rust-numpy.git", branch = "v0.18.graalpy" }
 ndarray = "0.13"
 onig = { version = "6.0", default-features = false }
 itertools = "0.9"
@@ -26,7 +26,7 @@ path = "./tokenizers-lib"
 
 [dev-dependencies]
 tempfile = "3.1"
-pyo3 = { version = "0.18.1", features = ["auto-initialize"] }
+pyo3 = { git = "https://github.com/timfel/pyo3.git", branch = "v0.18.1.graalpy", features = ["auto-initialize"] }
 
 [features]
 default = ["pyo3/extension-module"]
diff --git a/src/lib.rs b/src/lib.rs
index bf1adbd..89c34fe 100644
--- a/src/lib.rs
+++ b/src/lib.rs
@@ -48,14 +48,16 @@ extern "C" fn child_after_fork() {
 pub fn tokenizers(_py: Python, m: &PyModule) -> PyResult<()> {
     let _ = env_logger::try_init_from_env("TOKENIZERS_LOG");
 
+    // GraalPy change: Disable the atfork warning. This triggers a ton of false positives when
+    // jline calls stty and we don't support fork anyway
     // Register the fork callback
-    #[cfg(target_family = "unix")]
-    unsafe {
-        if !REGISTERED_FORK_CALLBACK {
-            libc::pthread_atfork(None, None, Some(child_after_fork));
-            REGISTERED_FORK_CALLBACK = true;
-        }
-    }
+    // #[cfg(target_family = "unix")]
+    // unsafe {
+    //     if !REGISTERED_FORK_CALLBACK {
+    //         libc::pthread_atfork(None, None, Some(child_after_fork));
+    //         REGISTERED_FORK_CALLBACK = true;
+    //     }
+    // }
 
     m.add_class::<tokenizer::PyTokenizer>()?;
     m.add_class::<tokenizer::PyAddedToken>()?;
diff --git a/tokenizers-lib/src/models/bpe/trainer.rs b/tokenizers-lib/src/models/bpe/trainer.rs
index 43ab848..55f95f8 100644
--- a/tokenizers-lib/src/models/bpe/trainer.rs
+++ b/tokenizers-lib/src/models/bpe/trainer.rs
@@ -518,15 +518,16 @@ impl BpeTrainer {
             let changes = top
                 .pos
                 .maybe_par_iter()
-                .flat_map(|i| {
-                    let w = &words[*i] as *const _ as *mut _;
+                .flat_map(|&i| {
+                    let word = &words[i] as *const _ as *mut Word;
                     // We can merge each of these words in parallel here because each position
                     // can be there only once (HashSet). So this is safe.
                     unsafe {
-                        let word: &mut Word = &mut (*w);
-                        word.merge(top.pair.0, top.pair.1, new_token_id)
+                        // let word: &mut Word = &mut (*word);
+                        (*word)
+                            .merge(top.pair.0, top.pair.1, new_token_id)
                             .into_iter()
-                            .map(|c| (c, *i))
+                            .map(|c| (c, i))
                             .collect::<Vec<_>>()
                     }
                 })
